---
title: "大模型量化、微调、推理框架对比"
author: "Charles"
description: ""
tags:
  - article
slug: "llm-framework"
pubDatetime: 2026-01-22T17:17:58.000+08:00
modDatetime: 2026-01-22T17:17:58.000+08:00
featured: false
draft: false
---

最近面试看到很多大模型相关的职位，学习一下模型层的知识。

### 主流推理/部署框架对比

| 框架名称          | 核心技术亮点                              | 吞吐量水平 | 易用性 | 支持硬件（主流成熟支持）                          | 典型适用场景                          | 主要短板                          |
|-------------------|-------------------------------------------|------------|--------|---------------------------------------------------|---------------------------------------|-----------------------------------|
| **vLLM**          | PagedAttention + 连续批处理               | 最高       | 高     | NVIDIA GPU（A100/H100/B200 最佳），AMD ROCm（部分），Intel Gaudi/XPU（实验） | 高并发生产服务、企业级 API 服务      | LoRA 支持较弱、不支持 CPU 主力   |
| **SGLang**        | RadixAttention + 结构化输出 + 零开销批处理 | 极高       | 中等   | NVIDIA GPU（Hopper/Blackwell 最佳）               | 复杂 Agent、结构化输出、函数调用任务 | 学习曲线稍陡、生态较新           |
| **TensorRT-LLM**  | NVIDIA TensorRT 深度内核优化 + MoE 支持   | 最高（单卡极致） | 中等（需编译） | 仅限 NVIDIA GPU（A100/H100/B200/L40S 等最优）     | 追求极致延迟/吞吐的生产环境          | 仅 NVIDIA、配置复杂、灵活性低    |
| **LMDeploy**      | TurboMind（极致优化） + PyTorch 后端      | 很高       | 高     | NVIDIA GPU、华为昇腾（Ascend）、部分 AMD          | 国产化场景、多模态、混合硬件部署      | 社区规模小于 vLLM                |
| **Ollama**        | 基于 llama.cpp 封装 + OpenAI API 兼容     | 中等       | 极高（一键） | CPU（x86/ARM）、NVIDIA GPU、AMD GPU（ROCm）、Apple Silicon（Metal） | 个人开发、本地快速原型、Mac 用户     | 吞吐量不高、不适合高并发         |
| **llama.cpp**     | GGUF 量化 + 多后端加速                    | 中等       | 中等   | CPU（x86/ARM）、NVIDIA（cuBLAS）、AMD（rocBLAS）、Apple Metal、Vulkan、SYCL 等 | 边缘设备、低端硬件、本地离线推理     | 吞吐量一般、分布式支持较弱       |
| **Hugging Face TGI** | FlashAttention + 连续批处理 + Rust 实现 | 很高       | 高     | NVIDIA GPU、AMD ROCm、Intel Gaudi、AWS Inferentia/Trainium | 研究实验、Hugging Face 生态快速部署 | 性能略逊于 vLLM/SGLang           |
| **XInference**    | 多后端集成（vLLM/Transformers/GGML 等）+ 分布式 + 多模态统一管理 | 很高（分布式场景更强） | 高     | NVIDIA GPU、AMD ROCm、华为昇腾（部分）、CPU、分布式集群 | 多模型统一管理、分布式部署、混合硬件、OpenAI 兼容服务 | 纯单机高吞吐不如 vLLM/SGLang 专注 |

**推理/部署框架推荐**  
- 追求最高吞吐量 + 高并发生产服务 → **首选 vLLM**  
- 需要极致单卡性能 + 纯 NVIDIA 环境 → **首选 TensorRT-LLM**  
- 复杂 Agent / 结构化输出 / 函数调用 → **首选 SGLang**  
- 国产化需求 / 昇腾硬件 → **首选 LMDeploy**  
- 多模型统一管理 / 分布式 / 混合硬件 → **首选 XInference**  
- 个人开发 / 本地快速上手 / Mac 用户 → **首选 Ollama**  
- 边缘设备 / CPU / 老旧硬件最大兼容 → **首选 llama.cpp**  
- Hugging Face 重度用户 / 追求稳定性 → **可选 TGI**

### 主流量化框架对比

| 框架/方法     | 核心技术亮点                        | 量化精度支持          | 精度损失       | 易用性 | 典型速度/显存收益 | 典型适用场景                       | 主要短板                     |
|---------------|-------------------------------------|-----------------------|----------------|--------|-------------------|------------------------------------|------------------------------|
| **GPTQ**      | 层级后训练量化（one-shot）          | INT4 / INT3 / INT2   | 较小           | 中等   | 极高（2-4×）      | 权重量化后离线部署                 | 不支持动态量化、对称性要求高 |
| **AWQ**       | 激活感知权重量化                    | INT4 主力            | 极小（SOTA之一）| 高     | 高                | 高精度 INT4 需求场景               | 主要针对权重量化             |
| **bitsandbytes** | NF4 / FP4 + 双重量化              | 8bit / 4bit(NF4)     | 小～中等       | 高（HF 集成） | 中～高            | 训练时量化 + 推理（QLoRA 常用）   | 推理速度不如专用内核         |
| **llama.cpp / GGUF** | 混合精度 + k-quants（Q4_K_M 等） | Q8_0 ~ Q2_K          | 中等～较大     | 中等   | 高（CPU/GPU 均可）| 本地/边缘/低配机器量化部署         | 精度损失相对明显             |
| **AutoAWQ**   | AWQ 的自动化实现 + HF 集成          | INT4 主力            | 极小           | 高     | 高                | 快速一键量化 HuggingFace 模型      | 依赖 AWQ 核心                |
| **HQQ**       | Half-Quadratic Quantization         | INT4/2bit 等         | 极小           | 中等   | 很高              | 追求极致低比特高精度场景           | 生态尚在快速发展            |

**量化框架推荐**  
- 追求最高精度 + INT4 推理 → **首选 AWQ** 或 **AutoAWQ**（简单好用）  
- 需要极致低比特（2bit/3bit）且精度尽量高 → **试试 HQQ**  
- QLoRA 微调时加载模型 → **首选 bitsandbytes**（与 HF 生态最无缝）  
- 本地/边缘设备 / CPU / 多平台部署 → **首选 llama.cpp GGUF**（Q4_K_M / Q5_K_M 最常用）  
- 希望一次性量化好后长期离线使用 → **GPTQ**（特别适合大模型）

### 主流微调框架对比

| 框架名称       | 核心技术亮点                              | 支持微调方法                  | 显存效率       | 易用性 | 速度提升       | 典型适用场景                           | 主要短板                       |
|----------------|-------------------------------------------|-------------------------------|----------------|--------|----------------|----------------------------------------|--------------------------------|
| **Unsloth**    | 手动 Triton kernel 优化 + 4bit 免费       | LoRA / QLoRA / DoRA 等        | 极高（2-5×）   | 极高   | 2-5× 更快      | 单卡/消费级 GPU 快速微调               | 目前主要支持主流模型           |
| **Axolotl**    | YAML 配置化 + 支持多种后端                | Full / LoRA / QLoRA / RLHF 等 | 高             | 高     | 较快           | 需要高度自定义配置的微调项目           | 初学者 YAML 学习成本稍高       |
| **LLaMA-Factory** | 一站式 WebUI + 支持 100+ 模型           | LoRA / QLoRA / Full / DPO 等  | 高             | 极高（有 UI） | 较快           | 研究/教学/快速实验                   | 极致性能不如 Unsloth           |
| **torchtune**  | 纯 PyTorch 原生、无抽象                   | LoRA / QLoRA / Full 等        | 高             | 中等～高 | 较快           | 想要纯 PyTorch 控制感的开发者          | 配置稍繁琐                     |
| **PEFT (HuggingFace)** | 官方参数高效微调库                      | LoRA / AdaLoRA / Prompt Tuning 等 | 高             | 高（与 HF 深度集成） | 中等           | 与 Transformers 生态无缝结合的项目     | 本身不提供训练加速             |

**微调框架推荐**  
- 单卡 / 消费级显卡 / 追求最快速度和最低显存 → 首选 Unsloth
- 想要带 WebUI / 一站式操作 / 快速实验 → 首选 LLaMA-Factory（支持100+模型、UI强大、中文生态友好）
- 需要高度自定义 / 支持 RLHF / 多阶段训练 → 首选 Axolotl
- 中文模型微调优先 / 较多中文数据集支持 → 首选 LLaMA-Factory
- 喜欢纯 PyTorch 风格 / 不想过多封装 → 可选 torchtune
- 普通 HF 项目 / 只想加 LoRA 不折腾 → 直接用 PEFT
