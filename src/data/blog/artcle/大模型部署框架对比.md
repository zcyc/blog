---
title: "大模型部署框架对比"
author: "Charles"
description: ""
tags:
  - article
slug: "llm-serving-framework"
pubDatetime: 2026-01-22T17:17:58.000+08:00
modDatetime: 2026-01-22T17:17:58.000+08:00
featured: false
draft: false
---

最近面试看到很多大模型开发职位要求了解大模型部署，所以在博客记录一下大模型部署相关的框架。

| 框架名称             | 核心技术亮点                               | 吞吐量水平   | 易用性     | 支持硬件（主流成熟支持）                              | 典型适用场景                           | 主要短板                     |
|----------------------|--------------------------------------------|--------------|------------|-------------------------------------------------------|----------------------------------------|------------------------------|
| **vLLM**            | PagedAttention + 连续批处理                | 最高         | 高         | NVIDIA GPU（A100 / H100 / B200 最佳），AMD ROCm（部分），Intel Gaudi / XPU（实验） | 高并发生产服务、企业级 API 服务        | LoRA 支持较弱、不支持 CPU 主力 |
| **SGLang**          | RadixAttention + 结构化输出 + 零开销批处理 | 极高         | 中等       | NVIDIA GPU（Hopper / Blackwell 最佳）                 | 复杂 Agent、结构化输出、函数调用任务   | 学习曲线稍陡、生态较新      |
| **TensorRT-LLM**    | NVIDIA TensorRT 深度内核优化 + MoE 支持    | 最高（单卡极致） | 中等（需编译）| 仅限 NVIDIA GPU（A100 / H100 / B200 / L40S 等最优）   | 追求极致延迟 / 吞吐的生产环境          | 仅 NVIDIA、配置复杂、灵活性低 |
| **LMDeploy**        | TurboMind（极致优化） + PyTorch 后端       | 很高         | 高         | NVIDIA GPU、华为昇腾（Ascend）、部分 AMD              | 国产化场景、多模态、混合硬件部署       | 社区规模小于 vLLM           |
| **Ollama**          | 基于 llama.cpp 封装 + OpenAI API 兼容      | 中等         | 极高（一键）| CPU（x86 / ARM）、NVIDIA GPU、AMD GPU（ROCm）、Apple Silicon（Metal） | 个人开发、本地快速原型、Mac 用户       | 吞吐量不高、不适合高并发    |
| **llama.cpp**       | GGUF 量化 + 多后端加速                     | 中等         | 中等       | CPU（x86 / ARM）、NVIDIA（cuBLAS）、AMD（rocBLAS）、Apple Metal、Vulkan、SYCL（Intel）等 | 边缘设备、低端硬件、本地离线推理       | 吞吐量一般、分布式支持较弱  |
| **Hugging Face TGI**| FlashAttention + 连续批处理 + Rust 实现    | 很高         | 高         | NVIDIA GPU、AMD ROCm、Intel Gaudi、AWS Inferentia / Trainium | 研究实验、Hugging Face 生态快速部署  | 性能略逊于 vLLM / SGLang    |
| **XInference**      | 多后端集成（vLLM / Transformers / GGML 等）+ 分布式 + 多模态统一管理 | 很高（分布式场景更强） | 高         | NVIDIA GPU、AMD ROCm、华为昇腾（部分）、CPU、分布式集群 | 多模型统一管理、分布式部署、混合硬件、OpenAI 兼容服务 | 纯单机高吞吐不如 vLLM / SGLang 专注、部分后端兼容性需调优 |

### 推荐

- 追求最高吞吐量 + 高并发生产服务 → 首选 **vLLM**（社区最活跃、生态最完善）
- 需要极致单卡性能 + NVIDIA 专属硬件 → 首选 **TensorRT-LLM**（延迟最低、硬件榨干最彻底）
- 复杂结构化输出 / Agent / 函数调用场景 → 首选 **SGLang**（控制力最强）
- 国产化需求 / 昇腾硬件 → 首选 **LMDeploy**（TurboMind 在国产卡上表现突出）
- 多模型统一管理 / 分布式部署 / 混合硬件环境 → 首选 **XInference**（后端灵活、支持 vLLM 加速 + 分布式扩展）
- 个人 / 本地 / 快速上手 / 无 GPU 或低配机器 → 首选 **Ollama**（最简单，一行命令跑起来）
- 极致边缘 / CPU / 老旧设备 / 最大硬件兼容 → 首选 **llama.cpp**（硬件支持最广）
- Hugging Face 重度用户 / 追求稳定性 → 可选 **TGI**