---
title: "大模型框架对比"
author: "Charles"
description: ""
tags:
  - article
slug: "llm-framework"
pubDatetime: 2026-01-22T17:17:58.000+08:00
modDatetime: 2026-01-22T17:17:58.000+08:00
featured: false
draft: false
---

上份工作的是 AI 项目，所以系统学习一下相关知识。

## 推理/部署框架对比

| 框架名称          | 核心技术亮点                              | 吞吐量水平 | 易用性 | 支持硬件（主流成熟支持）                          | 典型适用场景                          | 主要短板                          |
|-------------------|-------------------------------------------|------------|--------|---------------------------------------------------|---------------------------------------|-----------------------------------|
| **vLLM**          | PagedAttention + 连续批处理               | 最高       | 高     | NVIDIA GPU（A100/H100/B200 最佳），AMD ROCm（部分），Intel Gaudi/XPU（实验） | 高并发生产服务、企业级 API 服务      | LoRA 支持较弱、不支持 CPU 主力   |
| **SGLang**        | RadixAttention + 结构化输出 + 零开销批处理 | 极高       | 中等   | NVIDIA GPU（Hopper/Blackwell 最佳）               | 复杂 Agent、结构化输出、函数调用任务 | 学习曲线稍陡、生态较新           |
| **TensorRT-LLM**  | NVIDIA TensorRT 深度内核优化 + MoE 支持   | 最高（单卡极致） | 中等（需编译） | 仅限 NVIDIA GPU（A100/H100/B200/L40S 等最优）     | 追求极致延迟/吞吐的生产环境          | 仅 NVIDIA、配置复杂、灵活性低    |
| **LMDeploy**      | TurboMind（极致优化） + PyTorch 后端      | 很高       | 高     | NVIDIA GPU、华为昇腾（Ascend）、部分 AMD          | 国产化场景、多模态、混合硬件部署      | 社区规模小于 vLLM                |
| **Ollama**        | 基于 llama.cpp 封装 + OpenAI API 兼容     | 中等       | 极高（一键） | CPU（x86/ARM）、NVIDIA GPU、AMD GPU（ROCm）、Apple Silicon（Metal） | 个人开发、本地快速原型、Mac 用户     | 吞吐量不高、不适合高并发         |
| **llama.cpp**     | GGUF 量化 + 多后端加速                    | 中等       | 中等   | CPU（x86/ARM）、NVIDIA（cuBLAS）、AMD（rocBLAS）、Apple Metal、Vulkan、SYCL 等 | 边缘设备、低端硬件、本地离线推理     | 吞吐量一般、分布式支持较弱       |
| **Hugging Face TGI** | FlashAttention + 连续批处理 + Rust 实现 | 很高       | 高     | NVIDIA GPU、AMD ROCm、Intel Gaudi、AWS Inferentia/Trainium | 研究实验、Hugging Face 生态快速部署 | 性能略逊于 vLLM/SGLang           |
| **XInference**    | 多后端集成（vLLM/Transformers/GGML 等）+ 分布式 + 多模态统一管理 | 很高（分布式场景更强） | 高     | NVIDIA GPU、AMD ROCm、华为昇腾（部分）、CPU、分布式集群 | 多模型统一管理、分布式部署、混合硬件、OpenAI 兼容服务 | 纯单机高吞吐不如 vLLM/SGLang 专注 |

### 推理/部署框架推荐
- 追求最高吞吐量 + 高并发生产服务 → **首选 vLLM**  
- 需要极致单卡性能 + 纯 NVIDIA 环境 → **首选 TensorRT-LLM**  
- 复杂 Agent / 结构化输出 / 函数调用 → **首选 SGLang**  
- 国产化需求 / 昇腾硬件 → **首选 LMDeploy**  
- 多模型统一管理 / 分布式 / 混合硬件 → **首选 XInference**  
- 个人开发 / 本地快速上手 / Mac 用户 → **首选 Ollama**  
- 边缘设备 / CPU / 老旧硬件最大兼容 → **首选 llama.cpp**  
- Hugging Face 重度用户 / 追求稳定性 → **可选 TGI**

## 量化框架对比

| 框架/方法     | 核心技术亮点                        | 量化精度支持          | 精度损失       | 易用性 | 典型速度/显存收益 | 典型适用场景                       | 主要短板                     |
|---------------|-------------------------------------|-----------------------|----------------|--------|-------------------|------------------------------------|------------------------------|
| **GPTQ**      | 层级后训练量化（one-shot）          | INT4 / INT3 / INT2   | 较小           | 中等   | 极高（2-4×）      | 权重量化后离线部署                 | 不支持动态量化、对称性要求高 |
| **AWQ**       | 激活感知权重量化                    | INT4 主力            | 极小（SOTA之一）| 高     | 高                | 高精度 INT4 需求场景               | 主要针对权重量化             |
| **bitsandbytes** | NF4 / FP4 + 双重量化              | 8bit / 4bit(NF4)     | 小～中等       | 高（HF 集成） | 中～高            | 训练时量化 + 推理（QLoRA 常用）   | 推理速度不如专用内核         |
| **llama.cpp / GGUF** | 混合精度 + k-quants（Q4_K_M 等） | Q8_0 ~ Q2_K          | 中等～较大     | 中等   | 高（CPU/GPU 均可）| 本地/边缘/低配机器量化部署         | 精度损失相对明显             |
| **AutoAWQ**   | AWQ 的自动化实现 + HF 集成          | INT4 主力            | 极小           | 高     | 高                | 快速一键量化 HuggingFace 模型      | 依赖 AWQ 核心                |
| **HQQ**       | Half-Quadratic Quantization         | INT4/2bit 等         | 极小           | 中等   | 很高              | 追求极致低比特高精度场景           | 生态尚在快速发展            |

### 量化框架推荐
- 追求最高精度 + INT4 推理 → **首选 AWQ** 或 **AutoAWQ**（简单好用）  
- 需要极致低比特（2bit/3bit）且精度尽量高 → **试试 HQQ**  
- QLoRA 微调时加载模型 → **首选 bitsandbytes**（与 HF 生态最无缝）  
- 本地/边缘设备 / CPU / 多平台部署 → **首选 llama.cpp GGUF**（Q4_K_M / Q5_K_M 最常用）  
- 希望一次性量化好后长期离线使用 → **GPTQ**（特别适合大模型）

## 微调框架对比

| 框架名称       | 核心技术亮点                              | 支持微调方法                  | 显存效率       | 易用性 | 速度提升       | 典型适用场景                           | 主要短板                       |
|----------------|-------------------------------------------|-------------------------------|----------------|--------|----------------|----------------------------------------|--------------------------------|
| **Unsloth**    | 手动 Triton kernel 优化 + 4bit 免费       | LoRA / QLoRA / DoRA 等        | 极高（2-5×）   | 极高   | 2-5× 更快      | 单卡/消费级 GPU 快速微调               | 目前主要支持主流模型           |
| **Axolotl**    | YAML 配置化 + 支持多种后端                | Full / LoRA / QLoRA / RLHF 等 | 高             | 高     | 较快           | 需要高度自定义配置的微调项目           | 初学者 YAML 学习成本稍高       |
| **LLaMA-Factory** | 一站式 WebUI + 支持 100+ 模型           | LoRA / QLoRA / Full / DPO 等  | 高             | 极高（有 UI） | 较快           | 研究/教学/快速实验                   | 极致性能不如 Unsloth           |
| **torchtune**  | 纯 PyTorch 原生、无抽象                   | LoRA / QLoRA / Full 等        | 高             | 中等～高 | 较快           | 想要纯 PyTorch 控制感的开发者          | 配置稍繁琐                     |
| **PEFT (HuggingFace)** | 官方参数高效微调库                      | LoRA / AdaLoRA / Prompt Tuning 等 | 高             | 高（与 HF 深度集成） | 中等           | 与 Transformers 生态无缝结合的项目     | 本身不提供训练加速             |

### 微调框架推荐
- 单卡 / 消费级显卡 / 追求最快速度和最低显存 → 首选 Unsloth
- 想要带 WebUI / 一站式操作 / 快速实验 → 首选 LLaMA-Factory（支持100+模型、UI强大、中文生态友好）
- 需要高度自定义 / 支持 RLHF / 多阶段训练 → 首选 Axolotl
- 中文模型微调优先 / 较多中文数据集支持 → 首选 LLaMA-Factory
- 喜欢纯 PyTorch 风格 / 不想过多封装 → 可选 torchtune
- 普通 HF 项目 / 只想加 LoRA 不折腾 → 直接用 PEFT

## 智能体框架对比

| 框架名称                  | 开发者 / 组织          | 主要语言              | 核心焦点                        | 关键特性                                                                 | 优势                                          | 适用场景                                      |
|---------------------------|-------------------------|-----------------------|---------------------------------|--------------------------------------------------------------------------|-----------------------------------------------|-----------------------------------------------|
| **CrewAI**                | CrewAI Inc.            | Python                | 角色扮演式团队协作              | 角色/任务分配、层次化流程、人类反馈循环、并行执行；轻量高效             | **易用性极高**、**快速原型**、生产性能强；真实团队模拟 | 业务自动化、内容生成、团队式复杂任务分解      |
| **LlamaAgents**           | LlamaIndex Team        | Python                | 数据密集型 RAG 代理             | 强大检索增强、索引工具深度集成、多模态数据支持；易构建多代理系统       | **检索与数据处理能力突出**、知识密集任务优异  | RAG 应用、文档智能分析、知识库问答系统        |
| **LangGraph**             | LangChain              | Python                | 有状态图基工作流编排            | 节点/边循环、分支控制、状态持久化、可视化调试；继承 LangChain 生态      | **精确控制**复杂分支与错误处理；调试与可视化最佳 | 复杂状态工作流、持久化代理、多分支逻辑应用    |
| **OpenAI Agents SDK**     | OpenAI                 | Python (TypeScript 支持中) | 轻量级多代理工作流 + 守卫栏     | Handoffs、Guardrails、Tracing、Sessions；支持 100+ LLM、无状态到有状态   | **生产就绪**、**可观测性强**、易扩展；官方继任 Swarm | 生产级多代理系统、实时协作、高守卫需求场景    |
| **Microsoft Agent Framework** | Microsoft          | .NET / Python         | 企业级多代理编排                | 融合 AutoGen 与 Semantic Kernel；强化可观测性、合规性、Azure 深度集成   | **企业级稳健性强**、合规与安全优秀；实验→生产迁移顺畅 | 企业工作流、合规严格自动化、Azure 生态集成    |
| **Agent Development Kit (ADK)** | Google             | Python / Go / TypeScript / Java | 全生命周期开发与部署            | 模块化设计、多模态流式支持、CLI + 本地 UI、A2A 协议；深度集成 Gemini / Vertex AI | **生产就绪**、Google 生态集成紧密；多语言支持 | 生产级应用、多模态任务、复杂业务自动化、Google Cloud |
| **Strands Agents**        | AWS                    | Python                | 模型驱动自主代理                | 异步工具调用、MCP/A2A 支持、AWS 原生集成（Bedrock/Lambda/EC2）；几行代码建代理 | **简洁高效**、**生产部署极强**；AWS 生态无缝 | 企业级自动化、AWS 集成、高并发/模型驱动任务  |
| **Agno**                  | agno-agi               | Python                | 全栈多代理平台（框架 + 运行时） | AgentOS 云运行时、MCP/A2A、记忆/知识/评估（Agent as Judge）、水平扩展、无状态缩放 | **性能极致**、**隐私云部署强**；多模态/异步统一、高并发 | 企业多代理系统、安全敏感产品、长任务/规模化场景 |

### 智能体框架推荐
- 追求极致易用性 + 快速上线生产：首选 **CrewAI**（角色团队协作最直观）或 **OpenAI Agents SDK**（官方支持 + 强大 guardrails / tracing / handoffs）。
- 需要精细状态管理、复杂分支、循环与错误控制：**LangGraph**（图结构 + 持久化状态 + 可视化调试最强）。
- Azure 企业环境、合规与可观测性优先：**Microsoft Agent Framework**（企业稳健、Azure 原生）。
- Google Cloud / Vertex AI / Gemini 深度集成：**Agent Development Kit (ADK)**（全生命周期、多语言、多模态）。
- AWS 生态、高并发、模型驱动简洁开发：**Strands Agents**（几行代码 + Bedrock/Lambda 部署强）。
- 极致性能、无状态水平扩展、私有云多代理、安全/长任务：**Agno**（框架 + AgentOS 运行时一体，性能与规模领先）。

### 智能体框架不推荐
- **CAMEL**、**AgentScope**、**VoltAgent**、**MetaGPT**、**SuperAGI**、**Portia AI**：这些框架多用于研究、特定垂直或实验场景，生产采用率较低
- **OpenAI Swarm**：已被 Agents SDK 取代
- **AutoGen**：已融入 Microsoft Agent Framework
- **Semantic Kernel**：已融入 Microsoft Agent Framework